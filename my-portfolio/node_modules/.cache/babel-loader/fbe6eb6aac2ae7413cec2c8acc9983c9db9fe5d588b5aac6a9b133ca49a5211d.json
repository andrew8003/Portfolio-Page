{"ast":null,"code":"/**\r\n * @typedef {import('micromark-util-types').Construct} Construct\r\n * @typedef {import('micromark-util-types').Resolver} Resolver\r\n * @typedef {import('micromark-util-types').State} State\r\n * @typedef {import('micromark-util-types').Token} Token\r\n * @typedef {import('micromark-util-types').TokenizeContext} TokenizeContext\r\n * @typedef {import('micromark-util-types').Tokenizer} Tokenizer\r\n */\n\nimport { factorySpace } from 'micromark-factory-space';\nimport { markdownLineEnding } from 'micromark-util-character';\nimport { subtokenize } from 'micromark-util-subtokenize';\n/**\r\n * No name because it must not be turned off.\r\n * @type {Construct}\r\n */\nexport const content = {\n  tokenize: tokenizeContent,\n  resolve: resolveContent\n};\n\n/** @type {Construct} */\nconst continuationConstruct = {\n  tokenize: tokenizeContinuation,\n  partial: true\n};\n\n/**\r\n * Content is transparent: it’s parsed right now. That way, definitions are also\r\n * parsed right now: before text in paragraphs (specifically, media) are parsed.\r\n *\r\n * @type {Resolver}\r\n */\nfunction resolveContent(events) {\n  subtokenize(events);\n  return events;\n}\n\n/**\r\n * @this {TokenizeContext}\r\n * @type {Tokenizer}\r\n */\nfunction tokenizeContent(effects, ok) {\n  /** @type {Token | undefined} */\n  let previous;\n  return chunkStart;\n\n  /**\r\n   * Before a content chunk.\r\n   *\r\n   * ```markdown\r\n   * > | abc\r\n   *     ^\r\n   * ```\r\n   *\r\n   * @type {State}\r\n   */\n  function chunkStart(code) {\n    effects.enter(\"content\");\n    previous = effects.enter(\"chunkContent\", {\n      contentType: \"content\"\n    });\n    return chunkInside(code);\n  }\n\n  /**\r\n   * In a content chunk.\r\n   *\r\n   * ```markdown\r\n   * > | abc\r\n   *     ^^^\r\n   * ```\r\n   *\r\n   * @type {State}\r\n   */\n  function chunkInside(code) {\n    if (code === null) {\n      return contentEnd(code);\n    }\n\n    // To do: in `markdown-rs`, each line is parsed on its own, and everything\n    // is stitched together resolving.\n    if (markdownLineEnding(code)) {\n      return effects.check(continuationConstruct, contentContinue, contentEnd)(code);\n    }\n\n    // Data.\n    effects.consume(code);\n    return chunkInside;\n  }\n\n  /**\r\n   *\r\n   *\r\n   * @type {State}\r\n   */\n  function contentEnd(code) {\n    effects.exit(\"chunkContent\");\n    effects.exit(\"content\");\n    return ok(code);\n  }\n\n  /**\r\n   *\r\n   *\r\n   * @type {State}\r\n   */\n  function contentContinue(code) {\n    effects.consume(code);\n    effects.exit(\"chunkContent\");\n    previous.next = effects.enter(\"chunkContent\", {\n      contentType: \"content\",\n      previous\n    });\n    previous = previous.next;\n    return chunkInside;\n  }\n}\n\n/**\r\n * @this {TokenizeContext}\r\n * @type {Tokenizer}\r\n */\nfunction tokenizeContinuation(effects, ok, nok) {\n  const self = this;\n  return startLookahead;\n\n  /**\r\n   *\r\n   *\r\n   * @type {State}\r\n   */\n  function startLookahead(code) {\n    effects.exit(\"chunkContent\");\n    effects.enter(\"lineEnding\");\n    effects.consume(code);\n    effects.exit(\"lineEnding\");\n    return factorySpace(effects, prefixed, \"linePrefix\");\n  }\n\n  /**\r\n   *\r\n   *\r\n   * @type {State}\r\n   */\n  function prefixed(code) {\n    if (code === null || markdownLineEnding(code)) {\n      return nok(code);\n    }\n\n    // Always populated by defaults.\n\n    const tail = self.events[self.events.length - 1];\n    if (!self.parser.constructs.disable.null.includes('codeIndented') && tail && tail[1].type === \"linePrefix\" && tail[2].sliceSerialize(tail[1], true).length >= 4) {\n      return ok(code);\n    }\n    return effects.interrupt(self.parser.constructs.flow, nok, ok)(code);\n  }\n}","map":{"version":3,"names":["factorySpace","markdownLineEnding","subtokenize","content","tokenize","tokenizeContent","resolve","resolveContent","continuationConstruct","tokenizeContinuation","partial","events","effects","ok","previous","chunkStart","code","enter","contentType","chunkInside","contentEnd","check","contentContinue","consume","exit","next","nok","self","startLookahead","prefixed","tail","length","parser","constructs","disable","null","includes","type","sliceSerialize","interrupt","flow"],"sources":["C:/Users/Andrew/Documents/GitHub/Portfolio-Page/my-portfolio/node_modules/micromark-core-commonmark/lib/content.js"],"sourcesContent":["/**\r\n * @typedef {import('micromark-util-types').Construct} Construct\r\n * @typedef {import('micromark-util-types').Resolver} Resolver\r\n * @typedef {import('micromark-util-types').State} State\r\n * @typedef {import('micromark-util-types').Token} Token\r\n * @typedef {import('micromark-util-types').TokenizeContext} TokenizeContext\r\n * @typedef {import('micromark-util-types').Tokenizer} Tokenizer\r\n */\r\n\r\nimport { factorySpace } from 'micromark-factory-space';\r\nimport { markdownLineEnding } from 'micromark-util-character';\r\nimport { subtokenize } from 'micromark-util-subtokenize';\r\n/**\r\n * No name because it must not be turned off.\r\n * @type {Construct}\r\n */\r\nexport const content = {\r\n  tokenize: tokenizeContent,\r\n  resolve: resolveContent\r\n};\r\n\r\n/** @type {Construct} */\r\nconst continuationConstruct = {\r\n  tokenize: tokenizeContinuation,\r\n  partial: true\r\n};\r\n\r\n/**\r\n * Content is transparent: it’s parsed right now. That way, definitions are also\r\n * parsed right now: before text in paragraphs (specifically, media) are parsed.\r\n *\r\n * @type {Resolver}\r\n */\r\nfunction resolveContent(events) {\r\n  subtokenize(events);\r\n  return events;\r\n}\r\n\r\n/**\r\n * @this {TokenizeContext}\r\n * @type {Tokenizer}\r\n */\r\nfunction tokenizeContent(effects, ok) {\r\n  /** @type {Token | undefined} */\r\n  let previous;\r\n  return chunkStart;\r\n\r\n  /**\r\n   * Before a content chunk.\r\n   *\r\n   * ```markdown\r\n   * > | abc\r\n   *     ^\r\n   * ```\r\n   *\r\n   * @type {State}\r\n   */\r\n  function chunkStart(code) {\r\n    effects.enter(\"content\");\r\n    previous = effects.enter(\"chunkContent\", {\r\n      contentType: \"content\"\r\n    });\r\n    return chunkInside(code);\r\n  }\r\n\r\n  /**\r\n   * In a content chunk.\r\n   *\r\n   * ```markdown\r\n   * > | abc\r\n   *     ^^^\r\n   * ```\r\n   *\r\n   * @type {State}\r\n   */\r\n  function chunkInside(code) {\r\n    if (code === null) {\r\n      return contentEnd(code);\r\n    }\r\n\r\n    // To do: in `markdown-rs`, each line is parsed on its own, and everything\r\n    // is stitched together resolving.\r\n    if (markdownLineEnding(code)) {\r\n      return effects.check(continuationConstruct, contentContinue, contentEnd)(code);\r\n    }\r\n\r\n    // Data.\r\n    effects.consume(code);\r\n    return chunkInside;\r\n  }\r\n\r\n  /**\r\n   *\r\n   *\r\n   * @type {State}\r\n   */\r\n  function contentEnd(code) {\r\n    effects.exit(\"chunkContent\");\r\n    effects.exit(\"content\");\r\n    return ok(code);\r\n  }\r\n\r\n  /**\r\n   *\r\n   *\r\n   * @type {State}\r\n   */\r\n  function contentContinue(code) {\r\n    effects.consume(code);\r\n    effects.exit(\"chunkContent\");\r\n    previous.next = effects.enter(\"chunkContent\", {\r\n      contentType: \"content\",\r\n      previous\r\n    });\r\n    previous = previous.next;\r\n    return chunkInside;\r\n  }\r\n}\r\n\r\n/**\r\n * @this {TokenizeContext}\r\n * @type {Tokenizer}\r\n */\r\nfunction tokenizeContinuation(effects, ok, nok) {\r\n  const self = this;\r\n  return startLookahead;\r\n\r\n  /**\r\n   *\r\n   *\r\n   * @type {State}\r\n   */\r\n  function startLookahead(code) {\r\n    effects.exit(\"chunkContent\");\r\n    effects.enter(\"lineEnding\");\r\n    effects.consume(code);\r\n    effects.exit(\"lineEnding\");\r\n    return factorySpace(effects, prefixed, \"linePrefix\");\r\n  }\r\n\r\n  /**\r\n   *\r\n   *\r\n   * @type {State}\r\n   */\r\n  function prefixed(code) {\r\n    if (code === null || markdownLineEnding(code)) {\r\n      return nok(code);\r\n    }\r\n\r\n    // Always populated by defaults.\r\n\r\n    const tail = self.events[self.events.length - 1];\r\n    if (!self.parser.constructs.disable.null.includes('codeIndented') && tail && tail[1].type === \"linePrefix\" && tail[2].sliceSerialize(tail[1], true).length >= 4) {\r\n      return ok(code);\r\n    }\r\n    return effects.interrupt(self.parser.constructs.flow, nok, ok)(code);\r\n  }\r\n}"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAASA,YAAY,QAAQ,yBAAyB;AACtD,SAASC,kBAAkB,QAAQ,0BAA0B;AAC7D,SAASC,WAAW,QAAQ,4BAA4B;AACxD;AACA;AACA;AACA;AACA,OAAO,MAAMC,OAAO,GAAG;EACrBC,QAAQ,EAAEC,eAAe;EACzBC,OAAO,EAAEC;AACX,CAAC;;AAED;AACA,MAAMC,qBAAqB,GAAG;EAC5BJ,QAAQ,EAAEK,oBAAoB;EAC9BC,OAAO,EAAE;AACX,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,SAASH,cAAcA,CAACI,MAAM,EAAE;EAC9BT,WAAW,CAACS,MAAM,CAAC;EACnB,OAAOA,MAAM;AACf;;AAEA;AACA;AACA;AACA;AACA,SAASN,eAAeA,CAACO,OAAO,EAAEC,EAAE,EAAE;EACpC;EACA,IAAIC,QAAQ;EACZ,OAAOC,UAAU;;EAEjB;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACE,SAASA,UAAUA,CAACC,IAAI,EAAE;IACxBJ,OAAO,CAACK,KAAK,CAAC,SAAS,CAAC;IACxBH,QAAQ,GAAGF,OAAO,CAACK,KAAK,CAAC,cAAc,EAAE;MACvCC,WAAW,EAAE;IACf,CAAC,CAAC;IACF,OAAOC,WAAW,CAACH,IAAI,CAAC;EAC1B;;EAEA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACE,SAASG,WAAWA,CAACH,IAAI,EAAE;IACzB,IAAIA,IAAI,KAAK,IAAI,EAAE;MACjB,OAAOI,UAAU,CAACJ,IAAI,CAAC;IACzB;;IAEA;IACA;IACA,IAAIf,kBAAkB,CAACe,IAAI,CAAC,EAAE;MAC5B,OAAOJ,OAAO,CAACS,KAAK,CAACb,qBAAqB,EAAEc,eAAe,EAAEF,UAAU,CAAC,CAACJ,IAAI,CAAC;IAChF;;IAEA;IACAJ,OAAO,CAACW,OAAO,CAACP,IAAI,CAAC;IACrB,OAAOG,WAAW;EACpB;;EAEA;AACF;AACA;AACA;AACA;EACE,SAASC,UAAUA,CAACJ,IAAI,EAAE;IACxBJ,OAAO,CAACY,IAAI,CAAC,cAAc,CAAC;IAC5BZ,OAAO,CAACY,IAAI,CAAC,SAAS,CAAC;IACvB,OAAOX,EAAE,CAACG,IAAI,CAAC;EACjB;;EAEA;AACF;AACA;AACA;AACA;EACE,SAASM,eAAeA,CAACN,IAAI,EAAE;IAC7BJ,OAAO,CAACW,OAAO,CAACP,IAAI,CAAC;IACrBJ,OAAO,CAACY,IAAI,CAAC,cAAc,CAAC;IAC5BV,QAAQ,CAACW,IAAI,GAAGb,OAAO,CAACK,KAAK,CAAC,cAAc,EAAE;MAC5CC,WAAW,EAAE,SAAS;MACtBJ;IACF,CAAC,CAAC;IACFA,QAAQ,GAAGA,QAAQ,CAACW,IAAI;IACxB,OAAON,WAAW;EACpB;AACF;;AAEA;AACA;AACA;AACA;AACA,SAASV,oBAAoBA,CAACG,OAAO,EAAEC,EAAE,EAAEa,GAAG,EAAE;EAC9C,MAAMC,IAAI,GAAG,IAAI;EACjB,OAAOC,cAAc;;EAErB;AACF;AACA;AACA;AACA;EACE,SAASA,cAAcA,CAACZ,IAAI,EAAE;IAC5BJ,OAAO,CAACY,IAAI,CAAC,cAAc,CAAC;IAC5BZ,OAAO,CAACK,KAAK,CAAC,YAAY,CAAC;IAC3BL,OAAO,CAACW,OAAO,CAACP,IAAI,CAAC;IACrBJ,OAAO,CAACY,IAAI,CAAC,YAAY,CAAC;IAC1B,OAAOxB,YAAY,CAACY,OAAO,EAAEiB,QAAQ,EAAE,YAAY,CAAC;EACtD;;EAEA;AACF;AACA;AACA;AACA;EACE,SAASA,QAAQA,CAACb,IAAI,EAAE;IACtB,IAAIA,IAAI,KAAK,IAAI,IAAIf,kBAAkB,CAACe,IAAI,CAAC,EAAE;MAC7C,OAAOU,GAAG,CAACV,IAAI,CAAC;IAClB;;IAEA;;IAEA,MAAMc,IAAI,GAAGH,IAAI,CAAChB,MAAM,CAACgB,IAAI,CAAChB,MAAM,CAACoB,MAAM,GAAG,CAAC,CAAC;IAChD,IAAI,CAACJ,IAAI,CAACK,MAAM,CAACC,UAAU,CAACC,OAAO,CAACC,IAAI,CAACC,QAAQ,CAAC,cAAc,CAAC,IAAIN,IAAI,IAAIA,IAAI,CAAC,CAAC,CAAC,CAACO,IAAI,KAAK,YAAY,IAAIP,IAAI,CAAC,CAAC,CAAC,CAACQ,cAAc,CAACR,IAAI,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAACC,MAAM,IAAI,CAAC,EAAE;MAC/J,OAAOlB,EAAE,CAACG,IAAI,CAAC;IACjB;IACA,OAAOJ,OAAO,CAAC2B,SAAS,CAACZ,IAAI,CAACK,MAAM,CAACC,UAAU,CAACO,IAAI,EAAEd,GAAG,EAAEb,EAAE,CAAC,CAACG,IAAI,CAAC;EACtE;AACF","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}